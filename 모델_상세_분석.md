
1. 기본 모델 (submission_improved.csv) - 0.3493점 (최고)

현재 최고 성능 모델

데이터 구성
공행성쌍 탐색:
- 전체 가능한 쌍: 9,900개 (100×99)
- Pearson 상관계수 기준: `|r| ≥ 0.30`
- 최종 선택: **3,000개 쌍** (상위 고품질만)
- Lag 범위: 1~7개월 (각 쌍마다 최적 시차 탐색)

14개 특성 상세

1. 후행 품목 시계열 특성 (5개)
- `b_t`: 현재 월 거래액
- `b_t_1`: 1개월 전 거래액
- `b_t_2`: 2개월 전 거래액
- `b_ma3`: 최근 3개월 이동평균 (트렌드 파악)
- `b_change`: 전월 대비 변화율 `(b_t - b_t_1) / (b_t_1 + 1)`

2. 선행 품목 시계열 특성 (4개)
- `a_t_lag`: lag 시점의 선행 품목 거래액 (핵심)
- `a_t_lag_1`: lag+1 시점의 선행 품목 거래액
- `a_ma3`: 선행 품목의 3개월 이동평균
- `a_change`: 선행 품목의 변화율

3. 상호작용 특성 (1개)
- `ab_value_ratio`: 후행/선행 비율 `b_t / (a_t_lag + 1)`

4. 관계 특성 (4개)
- `max_corr`: 두 품목 간 최대 상관계수 (관계 강도)
- `best_lag`: 최적 시차 (몇 개월 후행?)
- `consistency`: 최근 6개월 상관관계 일관성 점수
- `corr_stability`: 1위/2위 상관계수 차이 (안정성)

학습 데이터 생성
- 3,000개 쌍 × 각 쌍당 평균 35개 시점 = 약 105,000개 샘플
- 시점별로 슬라이딩 윈도우 방식으로 학습 샘플 생성
- 예시: (2022-04 선행 → 2022-06 후행) 쌍이 lag=2라면
  - t=3: b[3], a[1] → target: b[4]
  - t=4: b[4], a[2] → target: b[5]
  - ... (반복)

XGBoost 하이퍼파라미터
```python
XGBRegressor(
    n_estimators=150,      # 트리 150개 (적당한 앙상블)
    max_depth=5,           # 트리 깊이 5 (과적합 방지)
    learning_rate=0.08,    # 학습률 (천천히 학습)
    subsample=0.85,        # 샘플 85%만 사용
    colsample_bytree=0.85, # 특성 85%만 사용
    min_child_weight=5,
    gamma=0.2,
    reg_alpha=0.5,         # L1 정규화
    reg_lambda=1.0,        # L2 정규화
    random_state=42
)
```

예측 과정
1. 2025년 7월 데이터로 2025년 8월 예측
2. 각 쌍마다 특성 추출 → XGBoost 예측
3. 후처리:
   - 음수 제거
   - 극단값 보정 (최근 6개월 max의 2배 제한)
   - 이동평균과 블렌딩 (극단 변화 완화)

성과
- 베이스라인 0.3201 → 0.3493 (+9.1%)
- 장점: 단순하고 안정적, 과적합 없음
- 단점: 추가 개선 여지 제한적


2. 고급 모델 (submission_advanced.csv) - 0.3348점 (실패)

복잡도 증가로 오히려 성능 저하

### 추가된 데이터
`train.csv`의 원본 컬럼 활용:
- `weight`: 품목별 무게 데이터
- `quantity`: 수량 데이터
- `seq`: 거래 빈도 (seq 개수)
- `hs4`: 품목 분류 코드 (같은 hs4 = 관련 품목)
- `type`: 거래 유형

### 28개 특성 상세

#### 기본 value 특성 (10개)
기존과 동일

#### weight 특성 (4개)
- `b_weight_t`: 후행 품목 현재 무게
- `a_weight_t_lag`: 선행 품목 lag 시점 무게
- `b_weight_change`: 후행 품목 무게 변화율
- `ab_weight_ratio`: 무게 비율

#### quantity 특성 (2개)
- `b_quantity_t`: 후행 품목 수량
- `a_quantity_t_lag`: 선행 품목 수량

#### trade_freq 특성 (2개)
- `b_freq_t`: 후행 품목 거래 빈도 (seq 개수)
- `a_freq_t_lag`: 선행 품목 거래 빈도

#### avg_trade_value 특성 (2개)
- `b_avg_val_t`: 후행 품목 평균 거래액
- `a_avg_val_t_lag`: 선행 품목 평균 거래액

#### 복합 특성 (2개)
- `value_per_weight_b`: 후행 품목 단위무게당 가치
- `value_per_weight_a`: 선행 품목 단위무게당 가치

#### HS4 유사성 포함 관계 특성 (4개)
- 기존 3개 + `hs4_similarity` 추가
- 같은 hs4 코드면 1.5배 가중치

### 공행성쌍 탐색 변화
- **3,500개 쌍** (500개 증가)
- HS4 유사성 반영: 같은 분류 코드면 우선순위 ↑
- 정렬 기준: `|상관계수| × hs4_similarity`

### 모델 복잡도 증가
```python
XGBRegressor(
    n_estimators=200,      # 150 → 200
    max_depth=6,           # 5 → 6 (더 복잡한 패턴)
    learning_rate=0.07,
    subsample=0.85,
    colsample_bytree=0.8,  # 0.85 → 0.8 (특성 많아져서)
    min_child_weight=5,
    gamma=0.2,
    reg_alpha=0.5,
    reg_lambda=1.0
)
```

### 학습 데이터
- 3,500개 쌍 × 평균 35시점 = **약 122,500개 샘플**
- 특성 14개 → 28개로 증가

### 문제점 분석

#### 1. weight/quantity 데이터 품질 이슈
- 많은 0값 존재 (거래 없는 달)
- 단위 불일치, 측정 오류 가능성
- 실제 value와 상관관계 낮음
- 예: 같은 value라도 weight가 들쭉날쭉

#### 2. 과적합 발생
- 특성 28개 → 모델이 노이즈까지 학습
- 훈련 데이터: 높은 정확도
- 테스트 데이터: 성능 하락
- 일반화 능력 상실

#### 3. 복잡도 증가의 역효과
- max_depth=6 → 더 깊은 트리 → 과적합 심화
- 추가 특성들이 오히려 혼란 야기
- "더 많은 정보 = 더 좋은 성능" ❌

### 결과
- 0.3493 → **0.3348** (-4.1%)
- **교훈**: 데이터 품질 > 데이터 양

---

## 3️⃣ 단순화 모델 (submission_simplified.csv) - **미테스트**

### 📌 고급 모델 실패 원인 제거 시도

### 전략
- weight/quantity/trade_freq/avg_trade_value **전부 제거**
- HS4 유사성만 유지 (이론적으로 유용)
- 14개 특성으로 복귀 (기본 모델 구조)

### 14개 특성
#### value 특성 (10개)
`b_t`, `b_t_1`, `b_t_2`, `b_ma3`, `b_change`
`a_t_lag`, `a_t_lag_1`, `a_ma3`, `a_change`, `ab_value_ratio`

#### 관계 특성 (4개)
`max_corr`, `best_lag`, `consistency`, `hs4_similarity` **(추가!)**

### 공행성쌍
- 3,500개 (고급 모델과 동일, HS4 반영)

### 모델 단순화
```python
XGBRegressor(
    n_estimators=150,      # 200 → 150 복귀
    max_depth=5,           # 6 → 5 복귀
    learning_rate=0.08,    # 0.07 → 0.08
    # ... 기본 모델과 유사
)
```

### 목적
- 0.3348에서 0.34~0.35로 복구
- HS4 유사성이 실제로 도움되는지 검증
- 500개 추가 쌍의 효과 확인

### 미테스트 이유
- 이미 초고급 모델로 실험 진행해버림
- 시간 관계상 스킵

---

## 4️⃣ 초고급 앙상블 모델 (submission_ultra.csv) - **0.293점** ❌❌

### 📌 모든 것을 동원했지만 최악의 결과

### 목표
- "0.5+ 점수 향상" 요청에 따라 전력 투구
- 시계열 분석의 모든 기법 총동원
- 최신 ML 기법 적용

---

## 65개 특성 초상세

### A. 시계열 특성 35개

#### 1. 기본 시계열 (7개)
```python
'b_t', 'b_t_1', 'b_t_2', 'b_t_3',
'a_t_lag', 'a_t_lag_1', 'a_t_lag_2'
```

#### 2. 다중 윈도우 이동평균 (6개)
```python
# 후행 품목
'b_ma3'   # 3개월 이동평균
'b_ma6'   # 6개월 이동평균
'b_ma12'  # 12개월 이동평균

# 선행 품목
'a_ma3', 'a_ma6', 'a_ma12'
```

#### 3. 다중 변화율 (6개)
```python
# 후행 품목
'b_change_1m'  # 1개월 변화율: (t - t-1) / t-1
'b_change_3m'  # 3개월 변화율: (t - t-3) / t-3
'b_change_6m'  # 6개월 변화율: (ma3 - ma3_prev) / ma3_prev

# 선행 품목
'a_change_1m', 'a_change_3m', 'a_change_6m'
```

#### 4. 고급 시계열 지표 (8개)
```python
# 가속도 (변화율의 변화율)
'b_accel' = (b_t - b_t_1) - (b_t_1 - b_t_2)
'a_accel' = (a_t_lag - a_t_lag_1) - (a_t_lag_1 - a_t_lag_2)

# 모멘텀 (단기 - 중기 차이)
'b_momentum' = b_ma3 - b_ma6
'a_momentum' = a_ma3 - a_ma6

# 변동성 (최근 6개월 표준편차)
'b_volatility' = std(b_value[-6:])
'a_volatility' = std(a_value[-6:])

# RSI (Relative Strength Index)
'b_rsi' = b_t / (b_ma12 + 1)
'a_rsi' = a_t_lag / (a_ma12 + 1)
```

#### 5. 상호작용 특성 (5개)
```python
'ab_ratio' = b_t / (a_t_lag + 1)
'ab_ma_ratio' = b_ma3 / (a_ma3 + 1)
'ab_change_ratio' = b_change_1m / (|a_change_1m| + 0.01)
'cross_momentum' = b_momentum × a_momentum
'cross_volatility' = b_volatility × a_volatility
```

#### 6. 추가 시점 (3개)
더 많은 과거 시점 참조로 패턴 포착 시도

---

### B. 품목별 정적 특성 20개

각 품목에 대해 **전체 43개월 데이터로 계산**한 통계:

#### 1. 기본 통계 (10개)
```python
'b_mean', 'a_mean'           # 전체 기간 평균
'b_std', 'a_std'             # 표준편차
'b_trend_3m', 'a_trend_3m'   # 최근 3개월 트렌드 (회귀 기울기)
'b_trend_6m', 'a_trend_6m'   # 최근 6개월 트렌드
'b_seasonality'              # 계절성 강도 (월별 std / 월별 mean)
'a_seasonality'
'b_trading_freq'             # 거래 빈도 (0이 아닌 값 비율)
'a_trading_freq'
```

#### 2. 안정성 지표 (4개)
```python
'b_stability' = 1 - (std / mean)  # 1에 가까울수록 안정
'a_stability'
'b_cv' = std / mean               # 변동계수 (낮을수록 안정)
'a_cv'
```

#### 3. 최근 활동성 (2개)
```python
'b_recent_active'  # 최근 6개월 중 거래 있는 달 비율
'a_recent_active'
```

#### 4. 분류 정보 (4개)
```python
'b_hs4', 'a_hs4'  # HS4 품목 분류 코드
```

---

### C. 관계 특성 10개

#### 1. 다중 상관계수 (4개)
```python
'max_corr'     # 전체 기간 최대 상관계수
'second_corr'  # 2위 상관계수
'recent_corr'  # 최근 12개월 상관계수
'mid_corr'     # 중간 12개월 상관계수 (24~12개월 전)
```

#### 2. 시차 및 안정성 (3개)
```python
'best_lag'        # 최적 시차 (1~12개월)
'consistency'     # 일관성 점수 (최근/과거 상관 부호 일치 여부)
'corr_stability'  # |max_corr - second_corr| (안정성)
```

#### 3. 유사성 (3개)
```python
'hs4_similarity'      # HS4 유사성 (1.0 or 1.5)
'feature_similarity'  # 품목 특성 코사인 유사도
'multi_lag_strength'  # 여러 lag에서 높은 상관 개수
```

---

## 공행성쌍 확장

### 탐색 전략
- **5,000개 쌍** (3,000 → 5,000, +67% 증가)
- **Lag 범위**: 1~12개월 (기존 1~7개월 확장)
- **상관계수 임계값**: 0.25 (0.30 → 0.25 완화)

### 복합 점수 계산
```python
composite_score = (
    |max_corr| × 
    consistency × 
    hs4_similarity × 
    feature_similarity × 
    (1 + 0.1 × multi_lag_strength)
)
```
- 단순 상관계수가 아닌 다차원 평가
- 상위 5,000개 선택

### Lag별 분포 예시
```
Lag 1: 850개
Lag 2: 920개
Lag 3: 780개
...
Lag 12: 120개
```

---

## 3개 모델 앙상블

### 1. XGBoost (가중치 40%)
```python
XGBRegressor(
    n_estimators=300,      # 150 → 300 증가
    max_depth=7,           # 5 → 7 (더 복잡한 패턴)
    learning_rate=0.05,    # 0.08 → 0.05 (천천히)
    subsample=0.8,
    colsample_bytree=0.7,  # 65개 특성이라 낮춤
    min_child_weight=3,
    gamma=0.3,
    reg_alpha=0.8,
    reg_lambda=1.2
)
```
- 가장 높은 비중
- 복잡한 비선형 패턴 학습

### 2. LightGBM (가중치 35%)
```python
LGBMRegressor(
    n_estimators=300,
    max_depth=8,
    learning_rate=0.05,
    subsample=0.8,
    colsample_bytree=0.7,
    min_child_samples=20,
    reg_alpha=0.8,
    reg_lambda=1.2
)
```
- leaf-wise 성장 (XGBoost와 다른 방식)
- 범주형 특성 처리 우수
- 학습 속도 빠름

### 3. CatBoost (가중치 25%)
```python
CatBoostRegressor(
    iterations=300,
    depth=7,
    learning_rate=0.05,
    l2_leaf_reg=3.0,
    random_seed=42
)
```
- 범주형 특성 자동 처리
- 과적합 방지 강화
- HS4 코드 같은 범주형에 유리

### 앙상블 예측
```python
y_pred = 0.4 × XGB_pred + 0.35 × LGB_pred + 0.25 × CAT_pred
```

---

## 학습 데이터

### 데이터 크기
- 5,000개 쌍 × 평균 31시점 = **약 155,000개 샘플**
- 특성: 65개
- 메모리: 약 80MB (float64 기준)

### Lookback 증가
- 기존: lag + 3개월 이상 데이터 필요
- 확장: lag + 12개월 이상 (ma12, trend_6m 등 계산)
- 초기 시점 샘플 감소 (2022년 데이터 일부 사용 불가)

---

## 예측 과정

### 1. 개별 모델 예측
```python
# 각 쌍에 대해
for pair in pairs_ultra:
    # 65개 특성 추출
    features = extract_features(pair, pivot_value, ultra_features)
    
    # 3개 모델 예측
    pred_xgb = model_xgb.predict(features)
    pred_lgb = model_lgb.predict(features)
    pred_cat = model_cat.predict(features)
```

### 2. 가중 평균
```python
y_pred = 0.4 * pred_xgb + 0.35 * pred_lgb + 0.25 * pred_cat
```

### 3. 고급 후처리

#### 3-시그마 룰
```python
recent_mean = mean(b_value[-6:])
recent_std = std(b_value[-6:])

upper_bound = recent_mean + 3 * recent_std
lower_bound = max(0, recent_mean - 3 * recent_std)

if y_pred > upper_bound:
    y_pred = upper_bound * 0.9
elif y_pred < lower_bound:
    y_pred = lower_bound * 1.1
```

#### 트렌드 반영
```python
trend = ultra_features[follower]['trend_3m']

if abs(trend) > 0.1:  # 강한 트렌드
    y_pred = y_pred * (1 + 0.3 * trend)
```

#### 극단값 보정
```python
if abs(y_pred - b_ma3) > b_ma3 * 3:
    y_pred = 0.5 * y_pred + 0.5 * b_ma3  # 이동평균과 블렌딩
```

---

## 실패 원인 심층 분석

### 1. 극심한 과적합
**증상:**
- 훈련 데이터: RMSE 매우 낮음 (거의 완벽)
- 테스트 데이터: 0.293 (처참)

**원인:**
- 65개 특성 → 모델이 훈련 데이터의 노이즈까지 암기
- 일반화 능력 완전 상실
- Cross-validation 점수와 실제 점수 괴리

**예시:**
```
CV RMSE (5-fold): 1200
실제 테스트 점수: 0.293 (RMSE 환산: ~3000)
```

### 2. 복잡도의 저주 (Curse of Dimensionality)

**문제:**
- 특성 14개 → 65개 (4.6배)
- 샘플 105,000개 → 155,000개 (1.5배)
- 샘플/특성 비율: 7,500 → 2,385 (1/3로 감소)

**결과:**
- 충분한 학습 데이터 부족
- 특성 간 다중공선성 심화
- 모델이 어떤 특성을 믿어야 할지 혼란

### 3. 시계열 특성의 역효과

#### ma12, trend_6m의 문제
```python
# 2025년 7월 시점에서
b_ma12 = mean(b_value[-12:])  # 2024-08 ~ 2025-07

# 하지만 예측 목표는 2025-08
# → 미래에 가까운 과거 정보 사용 = 정보 누설 위험
```

#### 트렌드/모멘텀/변동성의 불안정성
- 43개월 데이터로는 장기 트렌드 판단 부족
- 모멘텀은 노이즈에 민감
- 변동성은 품목마다 스케일 차이 큼

#### 계절성의 한계
```python
# 43개월 = 3.6년
# 계절성 판단하기에 충분하지 않음
# 특히 무역 데이터는 COVID-19, 경제 위기 등 이벤트 영향 큼
```

### 4. 5,000개 쌍의 품질 저하

**상위 3,000개 vs 하위 2,000개:**

| 순위 | 평균 상관계수 | 품질 |
|------|--------------|------|
| 1~1000 | 0.65 | 매우 높음 |
| 1001~3000 | 0.42 | 높음 |
| 3001~4000 | 0.31 | 보통 |
| 4001~5000 | 0.27 | 낮음 |

**문제:**
- 하위 2,000개는 약한 관계 (노이즈에 가까움)
- 약한 관계까지 학습 → 모델 혼란
- "쓰레기 데이터 in → 쓰레기 예측 out"

### 5. 앙상블의 과신

**기대:**
- 3개 다른 모델 → 다양한 관점 → 앙상블로 개선

**현실:**
- 3개 모델 모두 **같은 잘못된 특성**으로 학습
- 쓰레기 예측 3개를 평균 → 여전히 쓰레기
- 가중 평균해도 개선 없음

**예시:**
```
XGBoost 예측: 12000 (실제: 5000) - 2.4배 오차
LightGBM 예측: 13000 (실제: 5000) - 2.6배 오차
CatBoost 예측: 11500 (실제: 5000) - 2.3배 오차

앙상블 평균: 12,150 (실제: 5000) - 여전히 2.4배 오차
```

### 6. 특성 엔지니어링의 역설

**좋은 의도:**
- 더 많은 정보 → 더 정확한 예측

**실제 결과:**
- 관련 없는 특성들이 노이즈 생성
- 모델이 잘못된 패턴 학습

**예시:**
```python
# b_accel (가속도)
# 이론: 가속 패턴으로 미래 예측
# 실제: 무역 데이터는 가속도 패턴 약함, 노이즈만 증폭

# cross_volatility (변동성 곱)
# 이론: 두 품목 변동성 관계
# 실제: 단순 곱셈은 의미 없음, 스케일만 키움
```

---

## 결과 및 교훈

### 최종 점수
- 0.3493 → **0.293** (-16.0%)
- 사상 최악의 점수
- 베이스라인(0.3201)보다도 낮음

### 실행 시간
- 특성 생성: 약 15분
- 모델 학습: 약 25분 (3개 모델)
- 예측: 약 5분
- **총 45분** (기본 모델은 5분)

### 메모리 사용
- 약 2GB RAM (기본 모델은 500MB)

---

## 💡 핵심 교훈

### 1. "복잡함 ≠ 성능"의 증명

| 특성 개수 | 점수 | 차이 |
|----------|------|------|
| 14개 | 0.3493 | ✅ 최고 |
| 28개 | 0.3348 | -4.1% |
| 65개 | 0.293 | -16.0% ❌❌ |

**결론**: 무역 데이터는 **단순한 패턴** → **단순한 모델**이 최적!

### 2. 오컴의 면도날 (Occam's Razor)
> "같은 현상을 설명하는 두 이론이 있다면, 단순한 것을 선택하라"

- 14개 특성으로도 충분히 설명 가능
- 65개 특성은 불필요한 복잡성
- 단순함이 일반화의 열쇠

### 3. 데이터 품질 > 데이터 양

**weight/quantity 실패:**
- 많은 정보 ≠ 좋은 정보
- 노이즈 섞인 데이터는 오히려 해로움
- **깨끗한 14개 > 노이즈 섞인 28개**

### 4. 도메인 지식의 중요성

**무역 데이터 특성:**
- 계절성: 약함 (COVID, 경제 이벤트 영향 큼)
- 트렌드: 불안정 (경기 변동)
- 가속도/모멘텀: 의미 약함

**교훈:**
- 시계열 기법을 무작정 적용하면 안 됨
- 도메인 특성에 맞는 특성 선택 필요

### 5. 앙상블의 조건

**좋은 앙상블:**
- 다양한 모델 (서로 다른 오류 패턴)
- 좋은 데이터로 학습

**나쁜 앙상블:**
- 잘못된 데이터로 학습한 모델 여러 개
- "쓰레기 × 3 = 여전히 쓰레기"

### 6. 과적합 감지의 중요성

**경고 신호:**
- CV 점수 vs 실제 점수 괴리
- 훈련 오차 << 검증 오차
- 특성이 샘플에 비해 너무 많음

**예방책:**
- 정규화 강화
- 특성 선택 (feature selection)
- 더 단순한 모델 사용

---

## 5️⃣ 실용 모델 (submission_practical.csv) - **미실행**

### 📌 검증된 접근 + 최적화로 복구 시도

### 전략
초고급 모델 실패 후, **기본 모델로 회귀하되 최적화**

### 개선 포인트

#### 1. 공행성쌍 확대
- 3,000개 → **4,000개** (+33%)
- 상관계수 임계값: 0.30 → **0.28** (약간 완화)
- 품질 유지하면서 관계 확장

#### 2. 특성은 검증된 14개 유지
```python
# 변경 없음 - 기본 모델과 동일
'b_t', 'b_t_1', 'b_t_2', 'b_ma3', 'b_change',
'a_t_lag', 'a_t_lag_1', 'a_ma3', 'a_change', 'ab_value_ratio',
'max_corr', 'best_lag', 'consistency', 'corr_stability'
```

#### 3. 간단한 앙상블
- XGBoost(60%) + LightGBM(40%)
- 2개만 (3개는 과도)

#### 4. 하이퍼파라미터 최적화

**XGBoost:**
```python
n_estimators=250,      # 150 → 250 (약간 증가)
max_depth=5,           # 유지 (안정적)
learning_rate=0.06,    # 0.08 → 0.06 (더 천천히)
min_child_weight=6,    # 5 → 6 (과적합 방지 강화)
gamma=0.3,             # 0.2 → 0.3
reg_alpha=0.6,         # 0.5 → 0.6 (L1 정규화 강화)
reg_lambda=1.2         # 1.0 → 1.2 (L2 정규화 강화)
```

**LightGBM:**
```python
n_estimators=250,
max_depth=6,           # XGB보다 약간 깊게
learning_rate=0.06,
min_child_samples=25,  # 과적합 방지
reg_alpha=0.6,
reg_lambda=1.2
```

### 기대 효과
- 4,000개 쌍: 더 많은 관계 포착
- 최적화된 하이퍼파라미터: 과적합 방지
- 간단한 앙상블: 안정성 증가

### 예상 점수
- **0.37 ~ 0.40**
- 기본 모델(0.3493) 대비 +6~14%

### 미실행 이유
- 시간 부족
- 초고급 모델 실험에 집중

---

## 📊 전체 모델 비교표

| 모델 | 특성 | 쌍 | 모델 수 | 점수 | 차이 | 특징 |
|------|------|------|---------|------|------|------|
| Baseline | - | - | - | 0.3201 | - | 기준선 |
| **기본** ✅ | 14 | 3,000 | 1 | **0.3493** | +9.1% | 단순, 안정, 최고 |
| 고급 ❌ | 28 | 3,500 | 1 | 0.3348 | -4.1% | 노이즈 추가 |
| 단순화 ⚠️ | 14 | 3,500 | 1 | ? | ? | 미테스트 |
| 초고급 ❌❌ | 65 | 5,000 | 3 | 0.293 | -16.0% | 과적합 최악 |
| 실용 🎯 | 14 | 4,000 | 2 | 0.37~0.40? | +6~14%? | 최적화 (미실행) |

---

## 💭 최종 결론

### 성공 요인 (기본 모델)
1. ✅ 단순하고 해석 가능한 특성
2. ✅ 고품질 데이터만 사용 (value)
3. ✅ 적절한 모델 복잡도 (max_depth=5)
4. ✅ 강력한 정규화
5. ✅ 도메인 지식 반영 (공행성 개념)

### 실패 요인 (초고급 모델)
1. ❌ 과도한 특성 (65개)
2. ❌ 노이즈 데이터 포함 (weight, quantity)
3. ❌ 너무 깊은 모델 (max_depth=7~8)
4. ❌ 맹목적인 시계열 기법 적용
5. ❌ 약한 관계까지 포함 (5,000쌍)

### 핵심 메시지
> **"Keep It Simple, Stupid (KISS)"**
> 
> 무역 데이터 예측은 단순한 선형-비선형 관계로 충분히 설명 가능하다.
> 복잡한 모델은 오히려 노이즈를 학습하여 성능을 저하시킨다.
> 
> **단순함이 곧 강력함이다.**

---

## 📚 참고: 모델별 파일 및 코드 위치

### 파일
- `submission_improved.csv`: 기본 모델 (0.3493) ✅
- `submission_advanced.csv`: 고급 모델 (0.3348)
- `submission_simplified.csv`: 단순화 모델 (미생성)
- `submission_ultra.csv`: 초고급 모델 (0.293)
- `submission_practical.csv`: 실용 모델 (미생성)

### 노트북 위치
- 셀 1-17: 기본 모델
- 셀 15-17: 단순화 모델
- 셀 19-25: 초고급 모델
- 셀 27-31: 실용 모델 (코드만, 미실행)

---

**작성일**: 2025-11-15  
**프로젝트**: 무역 품목 공행성 예측  
**목표**: 0.3201 → 최대한 개선  
**최고 성과**: 0.3493 (기본 모델)  
**최악 실패**: 0.293 (초고급 모델)  
**교훈**: 단순함이 답이다.
