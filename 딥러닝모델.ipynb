{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6500086a",
   "metadata": {},
   "source": [
    "## 1. Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8debd8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# TensorFlow/Keras\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    from tensorflow import keras\n",
    "    from tensorflow.keras import layers\n",
    "    from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "    print(f\"TensorFlow version: {tf.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"TensorFlow not installed. Installing...\")\n",
    "    import subprocess\n",
    "    subprocess.check_call(['pip', 'install', 'tensorflow'])\n",
    "    import tensorflow as tf\n",
    "    from tensorflow import keras\n",
    "    from tensorflow.keras import layers\n",
    "    from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "    print(f\"TensorFlow version: {tf.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09317683",
   "metadata": {},
   "source": [
    "## 2. ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9178cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë°ì´í„° ë¡œë“œ\n",
    "train_raw = pd.read_csv('train.csv')\n",
    "print(f\"ì›ë³¸ ë°ì´í„°: {train_raw.shape}\")\n",
    "train_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7055e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# í”¼ë²— í…Œì´ë¸” ìƒì„± (100ê°œ í’ˆëª© Ã— 43ê°œì›”)\n",
    "pivot_by_month = train_raw.groupby(['item_id', 'year', 'month'])['value'].sum().reset_index()\n",
    "pivot_by_month['year_month'] = pivot_by_month['year'].astype(str) + '-' + pivot_by_month['month'].astype(str).str.zfill(2)\n",
    "pivot_value = pivot_by_month.pivot(index='item_id', columns='year_month', values='value').fillna(0)\n",
    "\n",
    "print(f\"í”¼ë²— í…Œì´ë¸”: {pivot_value.shape}\")\n",
    "print(f\"í’ˆëª© ìˆ˜: {len(pivot_value)}, ì›” ìˆ˜: {len(pivot_value.columns)}\")\n",
    "\n",
    "# ì‹œê³„ì—´ ë°ì´í„°ë¡œ ë³€í™˜ (shape: [n_items, n_months, 1])\n",
    "time_series_data = pivot_value.values  # [100, 43]\n",
    "n_items, n_months = time_series_data.shape\n",
    "\n",
    "print(f\"\\nì‹œê³„ì—´ ë°ì´í„° í˜•íƒœ: {time_series_data.shape}\")\n",
    "print(f\"í’ˆëª© ID: {pivot_value.index.tolist()[:10]}...\")\n",
    "print(f\"ì›” ë²”ìœ„: {pivot_value.columns[0]} ~ {pivot_value.columns[-1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adfb0015",
   "metadata": {},
   "source": [
    "## 3. ì‹œê³„ì—´ í•™ìŠµ ë°ì´í„° ìƒì„±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "464e6adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(data, lookback=12, forecast_horizon=1):\n",
    "    \"\"\"\n",
    "    ì‹œê³„ì—´ ì‹œí€€ìŠ¤ ìƒì„±\n",
    "    \n",
    "    Args:\n",
    "        data: [n_items, n_months] í˜•íƒœì˜ ì‹œê³„ì—´ ë°ì´í„°\n",
    "        lookback: ê³¼ê±° ëª‡ ê°œì›”ì„ ë³¼ ê²ƒì¸ê°€\n",
    "        forecast_horizon: ë¯¸ë˜ ëª‡ ê°œì›”ì„ ì˜ˆì¸¡í•  ê²ƒì¸ê°€\n",
    "    \n",
    "    Returns:\n",
    "        X: [n_samples, lookback, n_items] - ì…ë ¥ ì‹œí€€ìŠ¤\n",
    "        y: [n_samples, n_items] - íƒ€ê²Ÿê°’\n",
    "    \"\"\"\n",
    "    n_items, n_months = data.shape\n",
    "    X, y = [], []\n",
    "    \n",
    "    for i in range(lookback, n_months - forecast_horizon + 1):\n",
    "        # ê³¼ê±° lookback ê°œì›”ì˜ ëª¨ë“  í’ˆëª© ë°ì´í„°ë¥¼ ì…ë ¥ìœ¼ë¡œ\n",
    "        X.append(data[:, i-lookback:i].T)  # [lookback, n_items]\n",
    "        # forecast_horizon ê°œì›” í›„ì˜ ëª¨ë“  í’ˆëª© ê°’ì„ íƒ€ê²Ÿìœ¼ë¡œ\n",
    "        y.append(data[:, i+forecast_horizon-1])  # [n_items]\n",
    "    \n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# ì‹œí€€ìŠ¤ ìƒì„± (ê³¼ê±° 12ê°œì›” â†’ ë‹¤ìŒ 1ê°œì›” ì˜ˆì¸¡)\n",
    "lookback = 12\n",
    "X, y = create_sequences(time_series_data, lookback=lookback, forecast_horizon=1)\n",
    "\n",
    "print(f\"ì…ë ¥ ì‹œí€€ìŠ¤ X: {X.shape} [samples, timesteps, features]\")\n",
    "print(f\"íƒ€ê²Ÿ y: {y.shape} [samples, items]\")\n",
    "print(f\"\\nì´ ìƒ˜í”Œ ìˆ˜: {len(X)}ê°œ\")\n",
    "print(f\"ê° ìƒ˜í”Œ: ê³¼ê±° {lookback}ê°œì›” ë°ì´í„° â†’ ë‹¤ìŒ ë‹¬ {n_items}ê°œ í’ˆëª© ì˜ˆì¸¡\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e9cbbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë°ì´í„° ì •ê·œí™” (ê° í’ˆëª©ë³„ë¡œ)\n",
    "scalers = {}\n",
    "X_scaled = np.zeros_like(X)\n",
    "y_scaled = np.zeros_like(y)\n",
    "\n",
    "for i in range(n_items):\n",
    "    scaler = StandardScaler()\n",
    "    # Xì˜ ië²ˆì§¸ í’ˆëª© ì—´ ì •ê·œí™”\n",
    "    X_scaled[:, :, i] = scaler.fit_transform(X[:, :, i])\n",
    "    # yì˜ ië²ˆì§¸ í’ˆëª© ì •ê·œí™”\n",
    "    y_scaled[:, i] = scaler.transform(y[:, i].reshape(-1, 1)).flatten()\n",
    "    scalers[i] = scaler\n",
    "\n",
    "print(f\"ì •ê·œí™” ì™„ë£Œ: {len(scalers)}ê°œ í’ˆëª©ë³„ ìŠ¤ì¼€ì¼ëŸ¬ ìƒì„±\")\n",
    "print(f\"X_scaled ë²”ìœ„: [{X_scaled.min():.2f}, {X_scaled.max():.2f}]\")\n",
    "print(f\"y_scaled ë²”ìœ„: [{y_scaled.min():.2f}, {y_scaled.max():.2f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d502730c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/Validation ë¶„í•  (ì‹œê³„ì—´ì´ë¯€ë¡œ ìˆœì°¨ì  ë¶„í• )\n",
    "split_idx = int(len(X_scaled) * 0.8)\n",
    "\n",
    "X_train = X_scaled[:split_idx]\n",
    "y_train = y_scaled[:split_idx]\n",
    "X_val = X_scaled[split_idx:]\n",
    "y_val = y_scaled[split_idx:]\n",
    "\n",
    "print(f\"Train: X={X_train.shape}, y={y_train.shape}\")\n",
    "print(f\"Val:   X={X_val.shape}, y={y_val.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c27f1a3c",
   "metadata": {},
   "source": [
    "## 4. LSTM ëª¨ë¸ êµ¬ì¶•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29641fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_lstm_model(input_shape, output_dim, lstm_units=[128, 64], dropout=0.2):\n",
    "    \"\"\"\n",
    "    LSTM ê¸°ë°˜ ì‹œê³„ì—´ ì˜ˆì¸¡ ëª¨ë¸\n",
    "    \n",
    "    Args:\n",
    "        input_shape: (timesteps, features)\n",
    "        output_dim: ì˜ˆì¸¡í•  í’ˆëª© ìˆ˜\n",
    "        lstm_units: LSTM ë ˆì´ì–´ ìœ ë‹› ìˆ˜ ë¦¬ìŠ¤íŠ¸\n",
    "        dropout: ë“œë¡­ì•„ì›ƒ ë¹„ìœ¨\n",
    "    \"\"\"\n",
    "    model = keras.Sequential(name='LSTM_Forecaster')\n",
    "    \n",
    "    # ì²« ë²ˆì§¸ LSTM ë ˆì´ì–´\n",
    "    model.add(layers.LSTM(\n",
    "        lstm_units[0], \n",
    "        return_sequences=True if len(lstm_units) > 1 else False,\n",
    "        input_shape=input_shape,\n",
    "        name='lstm_1'\n",
    "    ))\n",
    "    model.add(layers.Dropout(dropout))\n",
    "    \n",
    "    # ì¶”ê°€ LSTM ë ˆì´ì–´ë“¤\n",
    "    for i, units in enumerate(lstm_units[1:], start=2):\n",
    "        return_seq = i < len(lstm_units)\n",
    "        model.add(layers.LSTM(units, return_sequences=return_seq, name=f'lstm_{i}'))\n",
    "        model.add(layers.Dropout(dropout))\n",
    "    \n",
    "    # Dense ë ˆì´ì–´\n",
    "    model.add(layers.Dense(64, activation='relu', name='dense_1'))\n",
    "    model.add(layers.Dropout(dropout))\n",
    "    model.add(layers.Dense(output_dim, activation='linear', name='output'))\n",
    "    \n",
    "    return model\n",
    "\n",
    "# ëª¨ë¸ ìƒì„±\n",
    "input_shape = (lookback, n_items)  # (12, 100)\n",
    "output_dim = n_items  # 100\n",
    "\n",
    "model = build_lstm_model(\n",
    "    input_shape=input_shape,\n",
    "    output_dim=output_dim,\n",
    "    lstm_units=[128, 64],\n",
    "    dropout=0.2\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d05f1d5",
   "metadata": {},
   "source": [
    "## 5. ëª¨ë¸ ì»´íŒŒì¼ ë° í•™ìŠµ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d59356eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ëª¨ë¸ ì»´íŒŒì¼\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss='mse',\n",
    "    metrics=['mae']\n",
    ")\n",
    "\n",
    "# ì½œë°± ì„¤ì •\n",
    "callbacks = [\n",
    "    EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=15,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=5,\n",
    "        min_lr=1e-6,\n",
    "        verbose=1\n",
    "    )\n",
    "]\n",
    "\n",
    "print(\"ëª¨ë¸ ì»´íŒŒì¼ ì™„ë£Œ\")\n",
    "print(\"Optimizer: Adam (lr=0.001)\")\n",
    "print(\"Loss: MSE\")\n",
    "print(\"Callbacks: EarlyStopping (patience=15), ReduceLROnPlateau\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562689f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ëª¨ë¸ í•™ìŠµ\n",
    "print(\"ëª¨ë¸ í•™ìŠµ ì‹œì‘...\\n\")\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=100,\n",
    "    batch_size=4,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\ní•™ìŠµ ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d1ab8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# í•™ìŠµ ê³¡ì„  ì‹œê°í™”\n",
    "plt.figure(figsize=(14, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Val Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss (MSE)')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['mae'], label='Train MAE')\n",
    "plt.plot(history.history['val_mae'], label='Val MAE')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MAE')\n",
    "plt.title('Training and Validation MAE')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"ìµœì¢… Train Loss: {history.history['loss'][-1]:.4f}\")\n",
    "print(f\"ìµœì¢… Val Loss: {history.history['val_loss'][-1]:.4f}\")\n",
    "print(f\"ìµœì¢… Val MAE: {history.history['val_mae'][-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c3766b2",
   "metadata": {},
   "source": [
    "## 6. ì˜ˆì¸¡ ë° ì—­ì •ê·œí™”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4edd8e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2025ë…„ 8ì›” ì˜ˆì¸¡ (ë§ˆì§€ë§‰ 12ê°œì›” ë°ì´í„° ì‚¬ìš©)\n",
    "last_sequence = time_series_data[:, -lookback:].T  # [12, 100]\n",
    "\n",
    "# ì •ê·œí™”\n",
    "last_sequence_scaled = np.zeros_like(last_sequence)\n",
    "for i in range(n_items):\n",
    "    last_sequence_scaled[:, i] = scalers[i].transform(last_sequence[:, i].reshape(-1, 1)).flatten()\n",
    "\n",
    "# ì˜ˆì¸¡\n",
    "last_sequence_scaled = last_sequence_scaled.reshape(1, lookback, n_items)\n",
    "prediction_scaled = model.predict(last_sequence_scaled, verbose=0)\n",
    "\n",
    "# ì—­ì •ê·œí™”\n",
    "prediction = np.zeros_like(prediction_scaled)\n",
    "for i in range(n_items):\n",
    "    prediction[:, i] = scalers[i].inverse_transform(prediction_scaled[:, i].reshape(-1, 1)).flatten()\n",
    "\n",
    "prediction = prediction.flatten()\n",
    "\n",
    "print(f\"2025ë…„ 8ì›” ì˜ˆì¸¡ê°’: {prediction.shape}\")\n",
    "print(f\"ì˜ˆì¸¡ê°’ ë²”ìœ„: [{prediction.min():.2f}, {prediction.max():.2f}]\")\n",
    "print(f\"ì˜ˆì¸¡ê°’ í†µê³„:\")\n",
    "print(pd.Series(prediction).describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2514baff",
   "metadata": {},
   "source": [
    "## 7. ê³µí–‰ì„±ìŒ íƒì§€ ë° ì œì¶œ íŒŒì¼ ìƒì„±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c84d8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê³µí–‰ì„±ìŒ íƒì§€ë¥¼ ìœ„í•œ ìƒê´€ê³„ìˆ˜ ê³„ì‚°\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "def find_comovement_pairs(predictions, historical_data, items, max_lag=7, corr_threshold=0.3, top_n=3000):\n",
    "    \"\"\"\n",
    "    ë”¥ëŸ¬ë‹ ì˜ˆì¸¡ê°’ì„ ê¸°ë°˜ìœ¼ë¡œ ê³µí–‰ì„±ìŒ íƒì§€\n",
    "    \"\"\"\n",
    "    pairs = []\n",
    "    \n",
    "    for i, leader in enumerate(items):\n",
    "        for j, follower in enumerate(items):\n",
    "            if i == j:\n",
    "                continue\n",
    "            \n",
    "            # ê³¼ê±° ë°ì´í„°ë¡œ ìƒê´€ê´€ê³„ í™•ì¸\n",
    "            best_corr = 0\n",
    "            best_lag = 1\n",
    "            \n",
    "            for lag in range(1, max_lag + 1):\n",
    "                if len(historical_data[i]) <= lag:\n",
    "                    continue\n",
    "                \n",
    "                x = historical_data[i][:-lag]\n",
    "                y = historical_data[j][lag:]\n",
    "                \n",
    "                if len(x) < 10 or np.std(x) == 0 or np.std(y) == 0:\n",
    "                    continue\n",
    "                \n",
    "                corr, _ = pearsonr(x, y)\n",
    "                if abs(corr) > abs(best_corr):\n",
    "                    best_corr = corr\n",
    "                    best_lag = lag\n",
    "            \n",
    "            if abs(best_corr) >= corr_threshold:\n",
    "                # ì˜ˆì¸¡ê°’ ì‚¬ìš©\n",
    "                pred_value = int(round(predictions[j]))\n",
    "                if pred_value < 0:\n",
    "                    pred_value = 0\n",
    "                \n",
    "                pairs.append({\n",
    "                    'leading_item_id': leader,\n",
    "                    'following_item_id': follower,\n",
    "                    'value': pred_value,\n",
    "                    'corr': abs(best_corr),\n",
    "                    'lag': best_lag\n",
    "                })\n",
    "    \n",
    "    # ìƒê´€ê³„ìˆ˜ ê¸°ì¤€ ì •ë ¬ í›„ ìƒìœ„ Nê°œ ì„ íƒ\n",
    "    df_pairs = pd.DataFrame(pairs)\n",
    "    if len(df_pairs) > top_n:\n",
    "        df_pairs = df_pairs.nlargest(top_n, 'corr')\n",
    "    \n",
    "    return df_pairs[['leading_item_id', 'following_item_id', 'value']]\n",
    "\n",
    "# ê³µí–‰ì„±ìŒ íƒì§€\n",
    "items = pivot_value.index.tolist()\n",
    "submission = find_comovement_pairs(\n",
    "    predictions=prediction,\n",
    "    historical_data=time_series_data,\n",
    "    items=items,\n",
    "    max_lag=7,\n",
    "    corr_threshold=0.3,\n",
    "    top_n=3000\n",
    ")\n",
    "\n",
    "print(f\"íƒì§€ëœ ê³µí–‰ì„±ìŒ: {len(submission):,}ê°œ\")\n",
    "print(f\"\\nì˜ˆì¸¡ê°’ í†µê³„:\")\n",
    "print(submission['value'].describe())\n",
    "submission.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b3cb0b",
   "metadata": {},
   "source": [
    "## 8. ì œì¶œ íŒŒì¼ ì €ì¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0459375",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì œì¶œ íŒŒì¼ ì €ì¥\n",
    "submission.to_csv('./submission_lstm.csv', index=False)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"ì‹œê³„ì—´ ë”¥ëŸ¬ë‹ ëª¨ë¸ (LSTM) ì™„ë£Œ!\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nì œì¶œ íŒŒì¼: submission_lstm.csv\")\n",
    "print(f\"ì˜ˆì¸¡ ìŒ ìˆ˜: {len(submission):,}ê°œ\")\n",
    "print(f\"\\nì˜ˆì¸¡ê°’ í†µê³„:\")\n",
    "print(submission['value'].describe())\n",
    "print(f\"\\n0ì¸ ì˜ˆì¸¡ê°’: {(submission['value'] == 0).sum()}ê°œ\")\n",
    "\n",
    "print(\"\\nëª¨ë¸ íŠ¹ì§•:\")\n",
    "print(\"1. LSTM ê¸°ë°˜ ì‹œê³„ì—´ ì˜ˆì¸¡\")\n",
    "print(\"2. 100ê°œ í’ˆëª©ì„ ë™ì‹œì— ëª¨ë¸ë§\")\n",
    "print(\"3. ê³¼ê±° 12ê°œì›” â†’ ë‹¤ìŒ 1ê°œì›” ì˜ˆì¸¡\")\n",
    "print(\"4. í’ˆëª© ê°„ ì‹œê°„ì  ì˜ì¡´ì„± í•™ìŠµ\")\n",
    "print(\"5. ìƒê´€ê´€ê³„ ê¸°ë°˜ ê³µí–‰ì„±ìŒ í•„í„°ë§\")\n",
    "\n",
    "print(\"\\në² ì´ìŠ¤ë¼ì¸ ëŒ€ë¹„ ê°œì„  ê¸°ëŒ€:\")\n",
    "print(\"- ê¸°ì¡´ XGBoost: 0.3454\")\n",
    "print(\"- LSTM ëª©í‘œ: 0.36-0.40+\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d75d60",
   "metadata": {},
   "source": [
    "## 9. í‰ê°€ ë©”íŠ¸ë¦­ ë¶„ì„ ë° ê°œì„  ì „ëµ\n",
    "\n",
    "**í˜„ì¬ ì ‘ê·¼ì˜ ë¬¸ì œ:**\n",
    "1. LSTMìœ¼ë¡œ ì˜ˆì¸¡ê°’ì€ ì–»ì—ˆì§€ë§Œ, ì—¬ì „íˆ correlationìœ¼ë¡œ ìŒì„ ì„ íƒ\n",
    "2. ì´ëŠ” ê¸°ì¡´ XGBoost ë°©ì‹ê³¼ ìŒ ì„ ì • ë¡œì§ì´ ë™ì¼\n",
    "3. LSTMì˜ ì‹œê³„ì—´ í•™ìŠµ ëŠ¥ë ¥ì„ ìŒ íƒì§€ì— í™œìš©í•˜ì§€ ëª»í•¨\n",
    "\n",
    "**í‰ê°€ ë©”íŠ¸ë¦­ ì´í•´:**\n",
    "- Score = 0.6 Ã— F1 + 0.4 Ã— (1 - NMAE)\n",
    "- F1: ìŒ íƒì§€ ì •í™•ë„ (ë” ì¤‘ìš”!)\n",
    "- NMAE: ì˜ˆì¸¡ê°’ ì •í™•ë„\n",
    "\n",
    "**ê°œì„  ì „ëµ:**\n",
    "1. **LSTM ì˜ˆì¸¡ ë³€í™”ëŸ‰ ê¸°ë°˜ ìŒ íƒì§€**: í’ˆëª© ê°„ ì˜ˆì¸¡ ë³€í™” íŒ¨í„´ ìœ ì‚¬ë„\n",
    "2. **Attention ë©”ì»¤ë‹ˆì¦˜**: í’ˆëª© ê°„ ìƒí˜¸ì‘ìš© í•™ìŠµ\n",
    "3. **Graph Neural Network**: í’ˆëª© ê´€ê³„ë¥¼ ê·¸ë˜í”„ë¡œ ëª¨ë¸ë§"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e834bc8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê°œì„ ëœ ìŒ íƒì§€: LSTM ì˜ˆì¸¡ ë³€í™”ëŸ‰ ê¸°ë°˜\n",
    "def find_comovement_pairs_v2(model, time_series_data, scalers, items, lookback=12, top_n=3000):\n",
    "    \"\"\"\n",
    "    LSTM ì˜ˆì¸¡ ë³€í™”ëŸ‰ì„ ì´ìš©í•œ ê³µí–‰ì„±ìŒ íƒì§€\n",
    "    \n",
    "    ì „ëµ: ê³¼ê±° Nê°œì›” ë™ì•ˆ ê° í’ˆëª©ì˜ ì˜ˆì¸¡ ë³€í™” íŒ¨í„´ì´ ìœ ì‚¬í•œ ìŒ ì°¾ê¸°\n",
    "    \"\"\"\n",
    "    n_items = len(items)\n",
    "    \n",
    "    # ê° ì‹œì ë³„ë¡œ ëª¨ë“  í’ˆëª© ì˜ˆì¸¡\n",
    "    predictions_over_time = []\n",
    "    \n",
    "    for t in range(lookback, len(time_series_data[0])):\n",
    "        sequence = time_series_data[:, t-lookback:t].T  # [12, 100]\n",
    "        \n",
    "        # ì •ê·œí™”\n",
    "        seq_scaled = np.zeros_like(sequence)\n",
    "        for i in range(n_items):\n",
    "            seq_scaled[:, i] = scalers[i].transform(sequence[:, i].reshape(-1, 1)).flatten()\n",
    "        \n",
    "        # ì˜ˆì¸¡\n",
    "        seq_scaled = seq_scaled.reshape(1, lookback, n_items)\n",
    "        pred_scaled = model.predict(seq_scaled, verbose=0)\n",
    "        \n",
    "        # ì—­ì •ê·œí™”\n",
    "        pred = np.zeros_like(pred_scaled)\n",
    "        for i in range(n_items):\n",
    "            pred[:, i] = scalers[i].inverse_transform(pred_scaled[:, i].reshape(-1, 1)).flatten()\n",
    "        \n",
    "        predictions_over_time.append(pred.flatten())\n",
    "    \n",
    "    predictions_over_time = np.array(predictions_over_time)  # [n_timepoints, n_items]\n",
    "    \n",
    "    print(f\"ì˜ˆì¸¡ ì‹œê³„ì—´ ìƒì„±: {predictions_over_time.shape}\")\n",
    "    \n",
    "    # ê° í’ˆëª©ì˜ ì˜ˆì¸¡ ë³€í™”ëŸ‰ ê³„ì‚°\n",
    "    changes = np.diff(predictions_over_time, axis=0)  # [n_timepoints-1, n_items]\n",
    "    \n",
    "    # í’ˆëª© ê°„ ë³€í™” íŒ¨í„´ ìƒê´€ê´€ê³„ ê³„ì‚°\n",
    "    pairs = []\n",
    "    \n",
    "    for i, leader in enumerate(items):\n",
    "        for j, follower in enumerate(items):\n",
    "            if i == j:\n",
    "                continue\n",
    "            \n",
    "            # ë³€í™” íŒ¨í„´ ìƒê´€ê³„ìˆ˜\n",
    "            if np.std(changes[:, i]) == 0 or np.std(changes[:, j]) == 0:\n",
    "                continue\n",
    "            \n",
    "            # lagë¥¼ ê³ ë ¤í•œ ìµœëŒ€ ìƒê´€ê³„ìˆ˜\n",
    "            best_corr = 0\n",
    "            best_lag = 0\n",
    "            \n",
    "            for lag in range(0, min(7, len(changes))):\n",
    "                if lag >= len(changes):\n",
    "                    continue\n",
    "                \n",
    "                x = changes[:-lag if lag > 0 else None, i]\n",
    "                y = changes[lag:, j]\n",
    "                \n",
    "                if len(x) < 5:\n",
    "                    continue\n",
    "                \n",
    "                corr = np.corrcoef(x, y)[0, 1]\n",
    "                if abs(corr) > abs(best_corr):\n",
    "                    best_corr = corr\n",
    "                    best_lag = lag\n",
    "            \n",
    "            if abs(best_corr) > 0.3:  # ì„ê³„ê°’\n",
    "                # ìµœì¢… ì˜ˆì¸¡ê°’ ì‚¬ìš©\n",
    "                final_pred = int(round(predictions_over_time[-1, j]))\n",
    "                if final_pred < 0:\n",
    "                    final_pred = 0\n",
    "                \n",
    "                pairs.append({\n",
    "                    'leading_item_id': leader,\n",
    "                    'following_item_id': follower,\n",
    "                    'value': final_pred,\n",
    "                    'corr': abs(best_corr),\n",
    "                    'lag': best_lag\n",
    "                })\n",
    "    \n",
    "    # ìƒê´€ê³„ìˆ˜ ê¸°ì¤€ ì •ë ¬ í›„ ìƒìœ„ Nê°œ ì„ íƒ\n",
    "    df_pairs = pd.DataFrame(pairs)\n",
    "    print(f\"íƒì§€ëœ ì „ì²´ ìŒ: {len(df_pairs):,}ê°œ\")\n",
    "    \n",
    "    if len(df_pairs) > top_n:\n",
    "        df_pairs = df_pairs.nlargest(top_n, 'corr')\n",
    "    \n",
    "    return df_pairs[['leading_item_id', 'following_item_id', 'value']]\n",
    "\n",
    "print(\"ê°œì„ ëœ ìŒ íƒì§€ ì „ëµ:\")\n",
    "print(\"1. ê³¼ê±° ì—¬ëŸ¬ ì‹œì ì—ì„œ LSTM ì˜ˆì¸¡ ìˆ˜í–‰\")\n",
    "print(\"2. ê° í’ˆëª©ì˜ ì˜ˆì¸¡ 'ë³€í™”ëŸ‰' ê³„ì‚°\")\n",
    "print(\"3. ë³€í™” íŒ¨í„´ì´ ìœ ì‚¬í•œ ìŒì„ ê³µí–‰ì„±ìŒìœ¼ë¡œ íŒë‹¨\")\n",
    "print(\"4. ë‹¨ìˆœ ê°’ ìƒê´€ê´€ê³„ê°€ ì•„ë‹Œ 'ë³€í™” íŒ¨í„´' ê¸°ë°˜\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98bbe9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê°œì„ ëœ ë°©ë²•ìœ¼ë¡œ ìŒ íƒì§€ (ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŒ)\n",
    "print(\"LSTM ê¸°ë°˜ ë³€í™” íŒ¨í„´ ë¶„ì„ ì‹œì‘...\\n\")\n",
    "\n",
    "submission_v2 = find_comovement_pairs_v2(\n",
    "    model=model,\n",
    "    time_series_data=time_series_data,\n",
    "    scalers=scalers,\n",
    "    items=items,\n",
    "    lookback=lookback,\n",
    "    top_n=3000\n",
    ")\n",
    "\n",
    "print(f\"\\nê°œì„ ëœ ë°©ë²•ìœ¼ë¡œ íƒì§€ëœ ê³µí–‰ì„±ìŒ: {len(submission_v2):,}ê°œ\")\n",
    "print(f\"\\nì˜ˆì¸¡ê°’ í†µê³„:\")\n",
    "print(submission_v2['value'].describe())\n",
    "submission_v2.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523274fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë‘ ë°©ë²• ë¹„êµ\n",
    "print(\"=\"*80)\n",
    "print(\"ë‘ ê°€ì§€ ìŒ íƒì§€ ë°©ë²• ë¹„êµ\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\n[ë°©ë²• 1] ê¸°ì¡´ Correlation ê¸°ë°˜:\")\n",
    "print(f\"  - ìŒ ìˆ˜: {len(submission):,}ê°œ\")\n",
    "print(f\"  - í‰ê·  ì˜ˆì¸¡ê°’: {submission['value'].mean():.2f}\")\n",
    "print(f\"  - ì¤‘ê°„ê°’: {submission['value'].median():.2f}\")\n",
    "\n",
    "print(f\"\\n[ë°©ë²• 2] LSTM ë³€í™” íŒ¨í„´ ê¸°ë°˜:\")\n",
    "print(f\"  - ìŒ ìˆ˜: {len(submission_v2):,}ê°œ\")\n",
    "print(f\"  - í‰ê·  ì˜ˆì¸¡ê°’: {submission_v2['value'].mean():.2f}\")\n",
    "print(f\"  - ì¤‘ê°„ê°’: {submission_v2['value'].median():.2f}\")\n",
    "\n",
    "# ë‘ ë°©ë²•ì˜ ìŒ ì¤‘ë³µë„ í™•ì¸\n",
    "pairs1 = set(zip(submission['leading_item_id'], submission['following_item_id']))\n",
    "pairs2 = set(zip(submission_v2['leading_item_id'], submission_v2['following_item_id']))\n",
    "\n",
    "overlap = len(pairs1 & pairs2)\n",
    "print(f\"\\nìŒ ì¤‘ë³µë„: {overlap:,}ê°œ ({overlap/len(pairs1)*100:.1f}%)\")\n",
    "print(f\"ë°©ë²• 1ë§Œì˜ ìŒ: {len(pairs1 - pairs2):,}ê°œ\")\n",
    "print(f\"ë°©ë²• 2ë§Œì˜ ìŒ: {len(pairs2 - pairs1):,}ê°œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cda1ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë°©ë²• 2ë¡œ ìµœì¢… ì œì¶œ íŒŒì¼ ì €ì¥\n",
    "submission_v2.to_csv('./submission_lstm_v2.csv', index=False)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"LSTM ë³€í™” íŒ¨í„´ ê¸°ë°˜ ëª¨ë¸ ì™„ë£Œ!\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nì œì¶œ íŒŒì¼: submission_lstm_v2.csv\")\n",
    "print(f\"ì˜ˆì¸¡ ìŒ ìˆ˜: {len(submission_v2):,}ê°œ\")\n",
    "print(f\"\\nì˜ˆì¸¡ê°’ í†µê³„:\")\n",
    "print(submission_v2['value'].describe())\n",
    "\n",
    "print(\"\\ní•µì‹¬ ì°¨ë³„ì :\")\n",
    "print(\"âœ“ ë‹¨ìˆœ ìƒê´€ê´€ê³„ â†’ LSTM í•™ìŠµ ê¸°ë°˜ ë³€í™” íŒ¨í„´\")\n",
    "print(\"âœ“ ì •ì  ë¶„ì„ â†’ ì‹œê³„ì—´ ë™ì  ë¶„ì„\")\n",
    "print(\"âœ“ í’ˆëª©ë³„ ë…ë¦½ â†’ 100ê°œ í’ˆëª© ë™ì‹œ ëª¨ë¸ë§\")\n",
    "print(\"âœ“ ì„ í˜• ê´€ê³„ â†’ ë¹„ì„ í˜• íŒ¨í„´ í•™ìŠµ\")\n",
    "\n",
    "print(\"\\nì˜ˆìƒ ê°œì„ :\")\n",
    "print(\"- F1 Score: ìŒ íƒì§€ ì •í™•ë„ í–¥ìƒ (ë³€í™” íŒ¨í„´ ìœ ì‚¬ë„)\")\n",
    "print(\"- NMAE: LSTM ì˜ˆì¸¡ê°’ ì •í™•ë„ í–¥ìƒ\")\n",
    "print(\"- ëª©í‘œ: 0.36-0.42\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9284ec2c",
   "metadata": {},
   "source": [
    "## 10. 0.45+ ë‹¬ì„±ì„ ìœ„í•œ ì „ëµ\n",
    "\n",
    "**í˜„ì‹¤:**\n",
    "- í˜„ì¬ ë°©ë²• í•œê³„: ~0.37\n",
    "- 1ë“± ì ìˆ˜: 0.45\n",
    "- ê²©ì°¨: +22% ê°œì„  í•„ìš”\n",
    "\n",
    "**í•„ìš”í•œ í˜ì‹ :**\n",
    "\n",
    "### 1. í’ˆëª© ë©”íƒ€ ì •ë³´ í™œìš©\n",
    "- item_id ì†ì„± ë¶„ì„ (íŒ¨í„´, ì¹´í…Œê³ ë¦¬ ì¶”ì •)\n",
    "- ìˆ«ì ë²”ìœ„ë¡œ ê·¸ë£¹í•‘ (101-110, 111-120 ë“±)\n",
    "- í’ˆëª© ê°„ êµ¬ì¡°ì  ê´€ê³„ íŒŒì•…\n",
    "\n",
    "### 2. ì‹œê³„ì—´ ë¶„í•´ (Decomposition)\n",
    "- Trend + Seasonality + Residual\n",
    "- ê³„ì ˆì„± íŒ¨í„´ ëª…ì‹œì  ëª¨ë¸ë§\n",
    "- ì£¼ê¸°ì„± íƒì§€ (ì›”ë³„, ë¶„ê¸°ë³„)\n",
    "\n",
    "### 3. Granger Causality\n",
    "- í†µê³„ì  ì¸ê³¼ê´€ê³„ í…ŒìŠ¤íŠ¸\n",
    "- Correlationë³´ë‹¤ ì •í™•í•œ ì„ í–‰-í›„í–‰ ê´€ê³„\n",
    "\n",
    "### 4. Transfer Entropy\n",
    "- ì •ë³´ ì´ë¡  ê¸°ë°˜ ì˜ì¡´ì„± ì¸¡ì •\n",
    "- ë¹„ì„ í˜• ê´€ê³„ í¬ì°©\n",
    "\n",
    "### 5. Ensemble Strategy\n",
    "- XGBoost + LSTM + Statistical Methods\n",
    "- Stacking with meta-learner\n",
    "- ë‹¤ì–‘í•œ threshold ì¡°í•©\n",
    "\n",
    "### 6. Feature Engineering\n",
    "```python\n",
    "- í’ˆëª©ë³„ ë³€ë™ì„± (volatility)\n",
    "- ìµœê·¼ ì¶”ì„¸ (recent trend)\n",
    "- ê°€ì†ë„ (acceleration)\n",
    "- ì£¼ê¸°ì„± ì ìˆ˜ (periodicity score)\n",
    "- í’ˆëª© ê°„ ê±°ë¦¬ ë©”íŠ¸ë¦­\n",
    "```\n",
    "\n",
    "**ë‹¤ìŒ ë‹¨ê³„ ì„ íƒì§€:**\n",
    "1. âœ… **ë¹ ë¥¸ ê°œì„ **: Granger Causality + ì•™ìƒë¸” (ëª©í‘œ 0.38-0.42)\n",
    "2. âš¡ **ì¤‘ê°„ ê°œì„ **: Transfer Entropy + ì‹œê³„ì—´ ë¶„í•´ (ëª©í‘œ 0.40-0.43)\n",
    "3. ğŸ¯ **í’€ìŠ¤íƒ**: ìœ„ ëª¨ë“  ë°©ë²• + ìˆ˜ì‘ì—… ë¶„ì„ (ëª©í‘œ 0.43-0.45+)\n",
    "\n",
    "ì–´ë–¤ ì „ëµìœ¼ë¡œ ê°ˆê¹Œìš”?"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
